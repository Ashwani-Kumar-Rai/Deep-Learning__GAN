{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gii0hJK4TEG3"
      },
      "source": [
        "We begin by importing PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "hXMCJplpTEG4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/exponential/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sWGfix7TEG5"
      },
      "source": [
        "## Tensors\n",
        "\n",
        "At its core, PyTorch is a library for processing tensors. A tensor is a number, vector, matrix or any n-dimensional array. Let's create a tensor with a single number:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tensor : any dimension \n",
        "# zero dimensions : number\n",
        "# one  dimensions : vector\n",
        "# two  dimensions : matrix\n",
        "# three dimensions : tensor\n",
        "\n",
        "# Deep learning me jobhe computation krte hai wo ,deep learning k upper ki jate hai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFb92nhzTEG5",
        "outputId": "7870006b-23b6-4bb6-88e2-4953f312e1b0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(4.)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Number : zero dimenstions\n",
        "t1 = torch.tensor(4.) # 4.0 is argument or input\n",
        "# 4. means floating point number\n",
        "t1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLy-dGxrTEG6"
      },
      "source": [
        "`4.` is a shorthand for `4.0`. It is used to indicate to Python (and PyTorch) that you want to create a floating point number. We can verify this by checking the `dtype` attribute of our tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "SjkM0FKmTEG7",
        "outputId": "e8c701a4-34be-4b58-861b-a58c46a6e611"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.float32"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t1.dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.int64"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "a2 = torch.tensor(4)\n",
        "a2.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkO0co7pTEG7"
      },
      "source": [
        "Let's try creating slightly more complex tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TLhYaVJNTEG8",
        "outputId": "750b3c48-5b7e-4671-b79a-865b659650a1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([1., 2., 3., 4.])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Vector : 1D \n",
        "t2 = torch.tensor([1., 2, 3, 4]) # if any floating point ,then all floating point\n",
        "t2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uowwAHICTEG8",
        "outputId": "0a4a4ca7-7536-4d70-efc5-e5b210022784"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 5.,  6.],\n",
              "        [ 7.,  8.],\n",
              "        [ 9., 10.]])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Matrix : 2 D  \n",
        "t3 = torch.tensor([[5., 6], # here 2 rows ,3 columns\n",
        "                   [7, 8], \n",
        "                   [9, 10]])\n",
        "t3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "rRwiw-GXTEG9",
        "outputId": "8605866b-5f81-49d5-8958-7997509c2317"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[11., 12., 13.],\n",
              "         [13., 14., 15.]],\n",
              "\n",
              "        [[15., 16., 17.],\n",
              "         [17., 18., 19.]]])"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 3-dimensional array : vizualize as matrix k peeche ek matrix rakha hua hai (LOL screeb 2D hai)\n",
        "t4 = torch.tensor([\n",
        "    [[11, 12, 13], \n",
        "     [13, 14, 15]], \n",
        "    [[15, 16, 17], \n",
        "     [17, 18, 19.]]])\n",
        "t4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hDATtrwUTEG9"
      },
      "source": [
        "Tensors can have any number of dimensions, and different lengths along each dimension. We can inspect the length along each dimension using the `.shape` property of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eLJHg2PvTEG9",
        "outputId": "e8a1c0ef-ffe9-49d3-ed8b-e3ff6078576a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(4.)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([])"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t1)\n",
        "t1.shape # shape -> elements and dimension "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "PbBW7GgRTEG-",
        "outputId": "c71f3748-e359-427b-b36c-4172155efe4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1., 2., 3., 4.])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t2)\n",
        "t2.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BzOdDuwFTEG-",
        "outputId": "1ced7e59-2a6a-4716-eb76-275c338783b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 5.,  6.],\n",
            "        [ 7.,  8.],\n",
            "        [ 9., 10.]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 2])"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t3)\n",
        "t3.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y92zthrhTEG-",
        "outputId": "2087b0c4-b10e-4afa-80b6-adc2bfd25a5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[11., 12., 13.],\n",
            "         [13., 14., 15.]],\n",
            "\n",
            "        [[15., 16., 17.],\n",
            "         [17., 18., 19.]]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([2, 2, 3])"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(t4)\n",
        "t4.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "expected sequence of length 4 at dim 2 (got 3)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Error : same (dimension/data type) hone chahie sabke\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m t4 \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([\n\u001b[1;32m      4\u001b[0m     [[\u001b[39m11\u001b[39;49m, \u001b[39m12\u001b[39;49m, \u001b[39m13\u001b[39;49m , \u001b[39m22\u001b[39;49m], \n\u001b[1;32m      5\u001b[0m      [\u001b[39m13\u001b[39;49m, \u001b[39m14\u001b[39;49m, \u001b[39m15\u001b[39;49m]], \n\u001b[1;32m      6\u001b[0m     [[\u001b[39m15\u001b[39;49m, \u001b[39m16\u001b[39;49m, \u001b[39m17\u001b[39;49m], \n\u001b[1;32m      7\u001b[0m      [\u001b[39m17\u001b[39;49m, \u001b[39m18\u001b[39;49m, \u001b[39m19.\u001b[39;49m]]])\n\u001b[1;32m      9\u001b[0m t4\n",
            "\u001b[0;31mValueError\u001b[0m: expected sequence of length 4 at dim 2 (got 3)"
          ]
        }
      ],
      "source": [
        "# Error : same (dimension/data type) hone chahie sabke\n",
        "\n",
        "t4 = torch.tensor([\n",
        "    [[11, 12, 13 , 22], \n",
        "     [13, 14, 15]], \n",
        "    [[15, 16, 17], \n",
        "     [17, 18, 19.]]])\n",
        "\n",
        "t4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCjDUJy6TEG_"
      },
      "source": [
        "## Tensor operations and gradients\n",
        "\n",
        "We can combine tensors with the usual arithmetic operations. Let's look an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# One of the benefit of pytorch is that we can do arithematic operation on tensor like a number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c_JEc4PyTEG_",
        "outputId": "1436290c-5374-461f-ca9c-c86dffcf4b6f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(tensor(3.), tensor(4., requires_grad=True), tensor(5., requires_grad=True))"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create tensors.\n",
        "x = torch.tensor(3.)\n",
        "w = torch.tensor(4., requires_grad=True) # requires_grad=True : means iska derivative calculate krke rakho\n",
        "b = torch.tensor(5., requires_grad=True) # grad means (gradient/derivative)\n",
        "x, w, b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUzDYoZmTEG_"
      },
      "source": [
        "We've created 3 tensors `x`, `w` and `b`, all numbers. `w` and `b` have an additional parameter `requires_grad` set to `True`. We'll see what it does in just a moment. \n",
        "\n",
        "Let's create a new tensor `y` by combining these tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KhIQ0WN9TEG_",
        "outputId": "ed2b49fe-1005-48fd-a72c-e72c8db3fe5b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(17., grad_fn=<AddBackward0>)"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Arithmetic operations\n",
        "y = w * x + b\n",
        "y \n",
        "# y ka derivate w k sath ,y ka derivate x k sath , y ka derivate b k sath  \n",
        "# pytorch ye derivate apne aap calculate krdeta hai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOVOY-83TEHA"
      },
      "source": [
        "As expected, `y` is a tensor with the value `3 * 4 + 5 = 17`. What makes PyTorch special is that we can automatically compute the derivative of `y` w.r.t. the tensors that have `requires_grad` set to `True` i.e. w and b. To compute the derivatives, we can call the `.backward` method on our result `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1VMX__c3TEHA"
      },
      "outputs": [],
      "source": [
        "# Compute derivatives\n",
        "y.backward()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBT3nA0aTEHA"
      },
      "source": [
        "The derivates of `y` w.r.t the input tensors are stored in the `.grad` property of the respective tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dRitjefXTEHA",
        "outputId": "7f90ddad-2110-4469-b448-e651a9f81ed6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "dy/dx: None\n",
            "dy/dw: tensor(6.)\n",
            "dy/db: tensor(3.)\n"
          ]
        }
      ],
      "source": [
        "# Display gradients\n",
        "print('dy/dx:', x.grad) # grad means (gradient/derivative)\n",
        "print('dy/dw:', w.grad)\n",
        "print('dy/db:', b.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dln-FLhvTEHB"
      },
      "source": [
        "As expected, `dy/dw` has the same value as `x` i.e. `3`, and `dy/db` has the value `1`. Note that `x.grad` is `None`, because `x` doesn't have `requires_grad` set to `True`. \n",
        "\n",
        "The \"grad\" in `w.grad` stands for gradient, which is another term for derivative, used mainly when dealing with matrices. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQbO17UhTEHB"
      },
      "source": [
        "## Interoperability with Numpy\n",
        "\n",
        "[Numpy](http://www.numpy.org/) is a popular open source library used for mathematical and scientific computing in Python. It enables efficient operations on large multi-dimensional arrays, and has a large ecosystem of supporting libraries:\n",
        "\n",
        "* [Matplotlib](https://matplotlib.org/) for plotting and visualization\n",
        "* [OpenCV](https://opencv.org/) for image and video processing\n",
        "* [Pandas](https://pandas.pydata.org/) for file I/O and data analysis\n",
        "\n",
        "Instead of reinventing the wheel, PyTorch interoperates really well with Numpy to leverage its existing ecosystem of tools and libraries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pcr_GKBhTEHB"
      },
      "source": [
        "Here's how we create an array in Numpy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9J_m7YSTEHB",
        "outputId": "418dff47-865d-4744-9966-1d735d4e03b5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "x = np.array([[1, 2], [3, 4.]])\n",
        "x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sczyTIIZTEHB"
      },
      "source": [
        "We can convert a Numpy array to a PyTorch tensor using `torch.from_numpy`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DzxXDQbrTEHC",
        "outputId": "b7f3e253-6d78-40b2-cfd9-5da2f84cdddd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 2.],\n",
              "        [3., 4.]], dtype=torch.float64)"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert the numpy array to a torch tensor.\n",
        "y = torch.from_numpy(x)\n",
        "y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGPtVOmzTEHC"
      },
      "source": [
        "Let's verify that the numpy array and torch tensor have similar data types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJ1fnWCnTEHC",
        "outputId": "4504bc66-50fb-492c-d065-30d8bd0bea40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(dtype('float64'), torch.float64)"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x.dtype, y.dtype"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXQaD3_5TEHC"
      },
      "source": [
        "We can convert a PyTorch tensor to a Numpy array using the `.numpy` method of a tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c30yElaTEHC",
        "outputId": "cf5607b5-3c6d-4156-8f59-617a102e7e03"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [3., 4.]])"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Convert a torch tensor to a numpy array\n",
        "z = y.numpy()\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Babr-1DvTEHC"
      },
      "source": [
        "The interoperability between PyTorch and Numpy is really important because most datasets you'll work with will likely be read and preprocessed as Numpy arrays."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
